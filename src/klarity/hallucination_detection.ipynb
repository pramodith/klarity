{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Detection using Entropy Metrics\n",
    "\n",
    "This notebook analyzes the effectiveness of entropy and semantic entropy for detecting hallucinations in LLM responses using the HaluEval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m142 packages\u001b[0m \u001b[2min 1.53s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m6 packages\u001b[0m \u001b[2min 5.92s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 593ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m18 packages\u001b[0m \u001b[2min 3.19s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2024.10.1\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mklarity\u001b[0m\u001b[2m==0.1.0 (from file:///C:/Users/pramo/ProgrammingProjects/klarity)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlitellm\u001b[0m\u001b[2m==1.61.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.61.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.63.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.36.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.22.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.21.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!uv add datasets litellm torch numpy pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mklarity\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EntropyAnalyzer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mklarity\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UncertaintyEstimator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import litellm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model2vec import StaticModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Tuple\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from klarity.core.analyzer import EntropyAnalyzer\n",
    "from klarity.estimator import UncertaintyEstimator\n",
    "from klarity.models import TokenInfo\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HaluEval dataset\n",
    "def get_halueval_dataset(split_name: str = \"qa\"):\n",
    "    dataset = datasets.load_dataset(\"notrichardren/HaluEval\", split_name)\n",
    "    print(f\"Dataset size: {len(dataset['train'])}\")\n",
    "    print(dataset['train'][0])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 10000/10000 [00:00<00:00, 45273.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n",
      "{'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\", 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\", 'right_answer': \"Arthur's Magazine\", 'hallucinated_answer': 'First for Women was started first.', 'task_type': 'QA'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['knowledge', 'question', 'right_answer', 'hallucinated_answer', 'task_type'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_halueval_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UncertaintyEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m entropy_analyzer \u001b[38;5;241m=\u001b[39m EntropyAnalyzer()\n\u001b[1;32m----> 2\u001b[0m uncertainty_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mUncertaintyEstimator\u001b[49m(top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, analyzer\u001b[38;5;241m=\u001b[39mentropy_analyzer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UncertaintyEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "entropy_analyzer = EntropyAnalyzer()\n",
    "uncertainty_estimator = UncertaintyEstimator(top_k=5, analyzer=entropy_analyzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(\n",
    "    text: str, \n",
    "    model: str = \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K\",\n",
    "    top_k: int = 5\n",
    ") -> str:\n",
    "    \"\"\"Get entropy metrics for a given response.\"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": text}],\n",
    "            logprobs=top_k,\n",
    "            echo=True\n",
    "        )\n",
    "        tokens = response.choices[0].logprobs.tokens\n",
    "        logprobs = response.choices[0].logprobs.token_logprobs\n",
    "        return response, response.choices[0].message.content, tokens, logprobs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vllm_response(\n",
    "    llm: LLM, \n",
    "    sampling_params: SamplingParams, \n",
    "    text: str,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    vllm_response = llm.generate([text], sampling_params)\n",
    "\n",
    "    generated_texts = []\n",
    "    num_generated_tokens = []\n",
    "    logprobs = []\n",
    "    tokens = []\n",
    "    token_infos = []\n",
    "\n",
    "    for prompt_ind in range(len(vllm_response)):\n",
    "        for sample_ind in range(len(vllm_response[prompt_ind].outputs)):\n",
    "            generated_texts.append(vllm_response[prompt_ind].outputs[sample_ind].text)\n",
    "            tokens.append(vllm_response[prompt_ind].outputs[sample_ind].tokens)\n",
    "            num_generated_tokens.append(len(vllm_response[prompt_ind].outputs[sample_ind].token_ids))\n",
    "            if get_logprobs:\n",
    "                lgps = []\n",
    "                for logprob in vllm_response[prompt_ind].outputs[sample_ind].logprobs:\n",
    "                    lgp = [lg.logprob for lg in logprob.values()]\n",
    "                    lgps.append(lgp)\n",
    "                logprobs.append(lgps)\n",
    "\n",
    "    return generated_texts, num_generated_tokens, logprobs, token_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\"What is the capital of France?\", \"What is the capital of Spain?\"]\n",
    "r, _, _, _ = get_response(sample_queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='91474dc559739567', created=1739979085, model='together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='eos', index=0, message=Message(content='The capital of France is Paris.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None), logprobs=ChoiceLogprobs(content=None, refusal=None, token_ids=[791, 6864, 315, 9822, 374, 12366, 13, 128009], tokens=['The', ' capital', ' of', ' France', ' is', ' Paris', '.', '<|eot_id|>'], token_logprobs=[-0.00010919571, -8.34465e-07, -5.9604645e-07, -1.1920929e-07, 0, -6.4373016e-06, -1.9907951e-05, -0.00062179565]))], usage=Usage(completion_tokens=8, prompt_tokens=42, total_tokens=50, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a subset of the dataset for testing\n",
    "sample_size = 100  # Adjust based on API limits and requirements\n",
    "results = []\n",
    "\n",
    "for idx in tqdm(range(sample_size)):\n",
    "    sample = dataset['train'][idx]\n",
    "    \n",
    "    # Process correct response\n",
    "    correct_entropy, correct_semantic_entropy = get_response_metrics(sample['correct'])\n",
    "    \n",
    "    # Process hallucinated response\n",
    "    hallu_entropy, hallu_semantic_entropy = get_response_metrics(sample['hallucinated'])\n",
    "    \n",
    "    results.append({\n",
    "        'query': sample['query'],\n",
    "        'correct_entropy': correct_entropy,\n",
    "        'correct_semantic_entropy': correct_semantic_entropy,\n",
    "        'hallu_entropy': hallu_entropy,\n",
    "        'hallu_semantic_entropy': hallu_semantic_entropy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(df: pd.DataFrame, entropy_threshold: float, semantic_threshold: float) -> Dict:\n",
    "    \"\"\"Evaluate the effectiveness of entropy metrics for hallucination detection.\"\"\"\n",
    "    # Predictions for correct responses (false positive rate)\n",
    "    correct_predictions = (\n",
    "        (df['correct_entropy'] > entropy_threshold) |\n",
    "        (df['correct_semantic_entropy'] > semantic_threshold)\n",
    "    )\n",
    "    false_positive_rate = correct_predictions.mean()\n",
    "    \n",
    "    # Predictions for hallucinated responses\n",
    "    hallu_predictions = (\n",
    "        (df['hallu_entropy'] > entropy_threshold) |\n",
    "        (df['hallu_semantic_entropy'] > semantic_threshold)\n",
    "    )\n",
    "    \n",
    "    true_labels = np.ones(len(df))\n",
    "    accuracy = accuracy_score(true_labels, hallu_predictions)\n",
    "    f1 = f1_score(true_labels, hallu_predictions)\n",
    "    \n",
    "    return {\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'hallucination_accuracy': accuracy,\n",
    "        'hallucination_f1': f1\n",
    "    }\n",
    "\n",
    "# Try different thresholds\n",
    "entropy_thresholds = np.linspace(0.5, 2.0, 10)\n",
    "semantic_thresholds = np.linspace(0.3, 1.5, 10)\n",
    "\n",
    "best_metrics = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "for e_thresh in entropy_thresholds:\n",
    "    for s_thresh in semantic_thresholds:\n",
    "        metrics = evaluate_metrics(results_df, e_thresh, s_thresh)\n",
    "        \n",
    "        # Score based on high hallucination detection and low false positives\n",
    "        score = metrics['hallucination_f1'] - metrics['false_positive_rate']\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_metrics = {\n",
    "                'entropy_threshold': e_thresh,\n",
    "                'semantic_threshold': s_thresh,\n",
    "                **metrics\n",
    "            }\n",
    "\n",
    "print(\"Best Results:\")\n",
    "for key, value in best_metrics.items():\n",
    "    print(f\"{key}: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

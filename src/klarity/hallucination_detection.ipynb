{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Detection using Entropy Metrics\n",
    "\n",
    "This notebook analyzes the effectiveness of entropy and semantic entropy for detecting hallucinations in LLM responses using the HaluEval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: uv: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!uv add datasets litellm torch numpy pandas scikit-learn tqdm vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/klarity/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-24 17:18:06,726\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import litellm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model2vec import StaticModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Tuple\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from klarity.core.analyzer import EntropyAnalyzer\n",
    "from klarity.estimator import UncertaintyEstimator\n",
    "from klarity.models import TokenInfo\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HaluEval dataset\n",
    "def get_halueval_dataset(split_name: str = \"qa\"\n",
    "):\n",
    "    dataset = datasets.load_dataset(\"notrichardren/HaluEval\", split_name)\n",
    "    print(f\"Dataset size: {len(dataset['train'])}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Sample the top 100 rows of the dataset\n",
    "ds = get_halueval_dataset()\n",
    "ds = ds['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_analyzer = EntropyAnalyzer()\n",
    "uncertainty_estimator = UncertaintyEstimator(top_k=5, analyzer=entropy_analyzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_litellm_response(\n",
    "    text: str, \n",
    "    model: str = \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K\",\n",
    "    top_k: int = 1\n",
    ") -> str:\n",
    "    \"\"\"Get entropy metrics for a given response.\"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": text}],\n",
    "            logprobs=top_k,\n",
    "            echo=True\n",
    "        )\n",
    "        tokens = response.choices[0].logprobs.tokens\n",
    "        logprobs = response.choices[0].logprobs.token_logprobs\n",
    "        return response, response.choices[0].message.content, tokens, logprobs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='917120c4cfd9474c', created=1740417644, model='together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='eos', index=0, message=Message(content='As of my knowledge cutoff in 2023, the President of the United States was Joe Biden. However, please note that my information may not be up to date, and I do not have real-time access to current events. \\n\\nTo get the most recent and accurate information, I recommend checking a reliable news source or the official website of the White House for the latest updates on the presidency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None), logprobs=ChoiceLogprobs(content=None, refusal=None, token_ids=[2170, 315, 856, 6677, 45379, 304, 220, 2366, 18, 11, 279, 4900, 315, 279, 3723, 4273, 574, 13142, 38180, 13, 4452, 11, 4587, 5296, 430, 856, 2038, 1253, 539, 387, 709, 311, 2457, 11, 323, 358, 656, 539, 617, 1972, 7394, 2680, 311, 1510, 4455, 13, 4815, 1271, 636, 279, 1455, 3293, 323, 13687, 2038, 11, 358, 7079, 13598, 264, 15062, 3754, 2592, 477, 279, 4033, 3997, 315, 279, 5929, 4783, 369, 279, 5652, 9013, 389, 279, 32858, 13, 128009], tokens=['As', ' of', ' my', ' knowledge', ' cutoff', ' in', ' ', '202', '3', ',', ' the', ' President', ' of', ' the', ' United', ' States', ' was', ' Joe', ' Biden', '.', ' However', ',', ' please', ' note', ' that', ' my', ' information', ' may', ' not', ' be', ' up', ' to', ' date', ',', ' and', ' I', ' do', ' not', ' have', ' real', '-time', ' access', ' to', ' current', ' events', '.', ' \\n\\n', 'To', ' get', ' the', ' most', ' recent', ' and', ' accurate', ' information', ',', ' I', ' recommend', ' checking', ' a', ' reliable', ' news', ' source', ' or', ' the', ' official', ' website', ' of', ' the', ' White', ' House', ' for', ' the', ' latest', ' updates', ' on', ' the', ' presidency', '.', '<|eot_id|>'], token_logprobs=[-0.875, -0.31445312, -0.0065307617, -0.46484375, -0.0074768066, -0.15136719, -0.578125, -0.00015640259, -1.3113022e-06, -0.00015258789, -0.09472656, -0.060058594, -4.7683716e-06, -9.655952e-06, -7.1525574e-07, -3.33786e-06, -0.5234375, -0.0014648438, -2.3841858e-07, -0.0031585693, -0.29492188, -0.004486084, -0.265625, -0.0028686523, -7.1525574e-05, -0.46484375, -0.16113281, -0.18554688, -0.25390625, -0.06982422, -0.006439209, -0.42773438, -4.5776367e-05, -0.69921875, -0.049804688, -1.0078125, -0.79296875, -0.0023651123, -0.014526367, -0.23730469, -1.1563301e-05, -0.328125, -0.07519531, -0.12695312, -0.059570312, -0.8828125, -1.015625, -0.5703125, -0.14746094, -0.002105713, -0.020874023, -0.76171875, -0.34960938, -0.0061950684, -0.00063323975, -0.100097656, -0.017700195, -0.35546875, -0.0067749023, -0.6328125, -0.023925781, -0.0047302246, -0.00029945374, -0.06225586, -0.3203125, -0.001701355, -0.18652344, -0.00049209595, -0.0002975464, -0.020141602, -9.059906e-06, -0.76171875, -0.020263672, -0.578125, -0.21386719, -0.16308594, -0.0018539429, -0.78515625, -0.037353516, -0.041748047]))], usage=Usage(completion_tokens=80, prompt_tokens=44, total_tokens=124, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt=[{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 February 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho is the president of the United States?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'logprobs': {'token_ids': [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1187, 7552, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 15546, 374, 279, 4872, 315, 279, 3723, 4273, 30, 128009, 128006, 78191, 128007, 271], 'tokens': ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '24', ' February', ' ', '202', '5', '\\n\\n', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'Who', ' is', ' the', ' president', ' of', ' the', ' United', ' States', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n'], 'token_logprobs': [None, -5.34375, -24.5, -2.359375, -15.5, -11.9375, -1.1875, -7.34375, -8.625, -0.034179688, -2.390625, -6.198883e-05, -2.796875, -5.625, -1.0625, -9.25, -8.8125, -0.00035476685, -0.29492188, -1.796875, -4.71875, -0.12890625, -0.022827148, -4.15625, -0.03564453, -20.75, -1.1920929e-07, -33.25, -0.67578125, -4.005432e-05, -7.875, -1.0625, -1.0390625, -4.09375, -0.015136719, -0.76953125, -0.59765625, -0.045654297, -0.67578125, -4.125, -1.1920929e-07, 0, 0, 0]}}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, _, _, _ = get_litellm_response(\"Who is the president of the United States?\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:18:09 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:18:13 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-24 17:18:13 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-24 17:18:13 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-24 17:18:14 model_runner.py:1110] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n",
      "INFO 02-24 17:18:14 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 02-24 17:18:14 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.24it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:18:14 model_runner.py:1115] Loading model weights took 0.6750 GB\n",
      "INFO 02-24 17:18:15 worker.py:267] Memory profiling takes 0.39 seconds\n",
      "INFO 02-24 17:18:15 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n",
      "INFO 02-24 17:18:15 worker.py:267] model weights take 0.67GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 34.21GiB.\n",
      "INFO 02-24 17:18:15 executor_base.py:111] # cuda blocks: 56045, # CPU blocks: 6553\n",
      "INFO 02-24 17:18:15 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 109.46x\n",
      "INFO 02-24 17:18:17 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:18:31 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.23 GiB\n",
      "INFO 02-24 17:18:31 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 16.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=128,\n",
    "    temperature=0.0,\n",
    "    logprobs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vllm_response(\n",
    "    llm: LLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    sampling_params: SamplingParams, \n",
    "    text: str,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    mean_entropy = []\n",
    "    mean_semantic_entropy = []\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\\\n",
    "            You are a question answering assistant. Respond with only the answer and no other context\"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    vllm_response = llm.generate(input_text, sampling_params)\n",
    "    answer = vllm_response[0].outputs[0].text\n",
    "    analysis_results = uncertainty_estimator.analyze_generation(vllm_response[0])\n",
    "    for token_metric in analysis_results.token_metrics:\n",
    "        mean_entropy.append(token_metric.raw_entropy)\n",
    "        mean_semantic_entropy.append(token_metric.semantic_entropy)\n",
    "    return answer, analysis_results, np.mean(mean_entropy), np.mean(mean_semantic_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 18.30it/s, est. speed input: 678.17 toks/s, output: 146.60 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sample_queries = [\"What is the capital of France?\", \"What is the capital of Spain?\"]\n",
    "answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, sample_queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Spain is Madrid.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\",\n",
       " 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
       " 'right_answer': \"Arthur's Magazine\",\n",
       " 'hallucinated_answer': 'First for Women was started first.',\n",
       " 'task_type': 'QA'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "predicted_answers = []\n",
    "mean_entropies = []\n",
    "mean_semantic_entropies = []\n",
    "correct_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\",\n",
       " 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
       " 'right_answer': \"Arthur's Magazine\",\n",
       " 'hallucinated_answer': 'First for Women was started first.',\n",
       " 'task_type': 'QA'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['knowledge', 'question', 'right_answer', 'hallucinated_answer', 'task_type']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m judge_response\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Update the main loop\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item)\n\u001b[1;32m     20\u001b[0m     correct_answers\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright_answer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2782\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2766\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2764\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2765\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2766\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2767\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2768\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2769\u001b[0m )\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:609\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    607\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 609\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:546\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['knowledge', 'question', 'right_answer', 'hallucinated_answer', 'task_type']\""
     ]
    }
   ],
   "source": [
    "def check_answers(predicted_answer, correct_answer):\n",
    "    # Check for exact match first\n",
    "    if predicted_answer.strip().lower() == correct_answer.strip().lower():\n",
    "        return True\n",
    "    \n",
    "    # If not an exact match, use JudgeLLM\n",
    "    prompt = f\"\"\"\n",
    "    Question: Are these two answers equivalent in meaning?\n",
    "    Answer 1: {predicted_answer}\n",
    "    Answer 2: {correct_answer}\n",
    "    Please respond with only 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, judge_response, _, _ = get_litellm_response(prompt)\n",
    "    return judge_response.strip().lower() == 'yes'\n",
    "\n",
    "# Update the main loop\n",
    "for item in tqdm(ds):\n",
    "    print(item)\n",
    "    correct_answers.append(item['right_answer'])\n",
    "    combined_text = f\"Context: {item['knowledge']}\\nQuestion: {item['question']}\\n Answer:\"\n",
    "    predicted_answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, combined_text)\n",
    "    predicted_answers.append(predicted_answer)\n",
    "    mean_entropies.append(mean_entropy)\n",
    "    mean_semantic_entropies.append(mean_semantic_entropy)\n",
    "    \n",
    "    # Check if the answers match\n",
    "    is_correct = check_answers(predicted_answer, item['right_answer'])\n",
    "    results.append(is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UncertaintyMetrics(raw_entropy=0.5193799723392782, semantic_entropy=0.5193799723392782, token_predictions=[TokenInfo(token='First', token_id=5345, logit=-0.4102168381214142, probability=0.6635063610636857, attention_score=None), TokenInfo(token='Arthur', token_id=27037, logit=-1.5352168083190918, probability=0.2154089836469735, attention_score=None), TokenInfo(token='B', token_id=50, logit=-2.910216808319092, probability=0.05446392035801202, attention_score=None), TokenInfo(token='Both', token_id=12857, logit=-4.535216808319092, probability=0.010724581795883955, attention_score=None), TokenInfo(token='Only', token_id=15017, logit=-5.035216808319092, probability=0.006504787671799594, attention_score=None)], insight=None, attention_metrics=None),\n",
       " UncertaintyMetrics(raw_entropy=0.011020957731614189, semantic_entropy=0.0006636399204034579, token_predictions=[TokenInfo(token=' for', token_id=327, logit=-0.0026856327895075083, probability=0.9973179702959892, attention_score=None), TokenInfo(token=' For', token_id=1068, logit=-6.127685546875, probability=0.002181624389459836, attention_score=None), TokenInfo(token='for', token_id=1710, logit=-8.815185546875, probability=0.00014846140286453328, attention_score=None), TokenInfo(token='.', token_id=30, logit=-10.315185546875, probability=3.3126216597023867e-05, attention_score=None), TokenInfo(token=' Women', token_id=6038, logit=-10.377685546875, probability=3.111920059283279e-05, attention_score=None)], insight=None, attention_metrics=None),\n",
       " UncertaintyMetrics(raw_entropy=0.002316005072438052, semantic_entropy=0.000573664230853115, token_predictions=[TokenInfo(token=' Women', token_id=6038, logit=-0.00039545822073705494, probability=0.9996046199625587, attention_score=None), TokenInfo(token=' Woman', token_id=20336, logit=-8.250395774841309, probability=0.00026115517819638484, attention_score=None), TokenInfo(token=' women', token_id=1554, logit=-9.875395774841309, probability=5.142450362689996e-05, attention_score=None), TokenInfo(token=' W', token_id=408, logit=-10.250395774841309, probability=3.534351000991581e-05, attention_score=None), TokenInfo(token='Women', token_id=17449, logit=-10.375395774841309, probability=3.119053811021834e-05, attention_score=None)], insight=None, attention_metrics=None),\n",
       " UncertaintyMetrics(raw_entropy=0.5904666284662318, semantic_entropy=0.5904666284662319, token_predictions=[TokenInfo(token='', token_id=2, logit=-0.4553496837615967, probability=0.6342261505814846, attention_score=None), TokenInfo(token=' was', token_id=436, logit=-1.4553496837615967, probability=0.23331876185223158, attention_score=None), TokenInfo(token='.', token_id=30, logit=-2.8303496837615967, probability=0.05899222144038213, attention_score=None), TokenInfo(token=' is', token_id=314, logit=-3.4553496837615967, probability=0.03157626071968755, attention_score=None), TokenInfo(token=',', token_id=28, logit=-4.580349922180176, probability=0.010251308508494402, attention_score=None)], insight=None, attention_metrics=None)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].token_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(df: pd.DataFrame, entropy_threshold: float, semantic_threshold: float) -> Dict:\n",
    "    \"\"\"Evaluate the effectiveness of entropy metrics for hallucination detection.\"\"\"\n",
    "    # Predictions for correct responses (false positive rate)\n",
    "    correct_predictions = (\n",
    "        (df['correct_entropy'] > entropy_threshold) |\n",
    "        (df['correct_semantic_entropy'] > semantic_threshold)\n",
    "    )\n",
    "    false_positive_rate = correct_predictions.mean()\n",
    "    \n",
    "    # Predictions for hallucinated responses\n",
    "    hallu_predictions = (\n",
    "        (df['hallu_entropy'] > entropy_threshold) |\n",
    "        (df['hallu_semantic_entropy'] > semantic_threshold)\n",
    "    )\n",
    "    \n",
    "    true_labels = np.ones(len(df))\n",
    "    accuracy = accuracy_score(true_labels, hallu_predictions)\n",
    "    f1 = f1_score(true_labels, hallu_predictions)\n",
    "    \n",
    "    return {\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'hallucination_accuracy': accuracy,\n",
    "        'hallucination_f1': f1\n",
    "    }\n",
    "\n",
    "# Try different thresholds\n",
    "entropy_thresholds = np.linspace(0.5, 2.0, 10)\n",
    "semantic_thresholds = np.linspace(0.3, 1.5, 10)\n",
    "\n",
    "best_metrics = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "for e_thresh in entropy_thresholds:\n",
    "    for s_thresh in semantic_thresholds:\n",
    "        metrics = evaluate_metrics(results_df, e_thresh, s_thresh)\n",
    "        \n",
    "        # Score based on high hallucination detection and low false positives\n",
    "        score = metrics['hallucination_f1'] - metrics['false_positive_rate']\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_metrics = {\n",
    "                'entropy_threshold': e_thresh,\n",
    "                'semantic_threshold': s_thresh,\n",
    "                **metrics\n",
    "            }\n",
    "\n",
    "print(\"Best Results:\")\n",
    "for key, value in best_metrics.items():\n",
    "    print(f\"{key}: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

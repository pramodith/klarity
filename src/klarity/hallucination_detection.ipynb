{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Detection using Entropy Metrics\n",
    "\n",
    "This notebook analyzes the effectiveness of entropy and semantic entropy for detecting hallucinations in LLM responses using the HaluEval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: uv: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!uv add datasets litellm torch numpy pandas scikit-learn tqdm vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import litellm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model2vec import StaticModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Tuple\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from klarity.core.analyzer import EntropyAnalyzer\n",
    "from klarity.estimator import UncertaintyEstimator\n",
    "from klarity.models import TokenInfo\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HaluEval dataset\n",
    "def get_halueval_dataset(split_name: str = \"qa\"\n",
    "):\n",
    "    dataset = datasets.load_dataset(\"notrichardren/HaluEval\", split_name)\n",
    "    print(f\"Dataset size: {len(dataset['train'])}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Sample the top 100 rows of the dataset\n",
    "ds = get_halueval_dataset()\n",
    "ds = ds['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_analyzer = EntropyAnalyzer()\n",
    "uncertainty_estimator = UncertaintyEstimator(top_k=5, analyzer=entropy_analyzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_litellm_response(\n",
    "    text: str, \n",
    "    model: str = \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K\",\n",
    "    top_k: int = 1\n",
    ") -> str:\n",
    "    \"\"\"Get entropy metrics for a given response.\"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": text}],\n",
    "            logprobs=top_k,\n",
    "            echo=True\n",
    "        )\n",
    "        tokens = response.choices[0].logprobs.tokens\n",
    "        logprobs = response.choices[0].logprobs.token_logprobs\n",
    "        return response, response.choices[0].message.content, tokens, logprobs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='91713f0f29d7e993', created=1740418885, model='together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='eos', index=0, message=Message(content='As of my knowledge cutoff in 2023, the President of the United States was Joe Biden. However, please note that my information may not be up to date, and I do not have real-time access to current events. \\n\\nTo get the most recent and accurate information, I recommend checking a reliable news source or the official website of the White House for the latest updates on the presidency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None), logprobs=ChoiceLogprobs(content=None, refusal=None, token_ids=[2170, 315, 856, 6677, 45379, 304, 220, 2366, 18, 11, 279, 4900, 315, 279, 3723, 4273, 574, 13142, 38180, 13, 4452, 11, 4587, 5296, 430, 856, 2038, 1253, 539, 387, 709, 311, 2457, 11, 323, 358, 656, 539, 617, 1972, 7394, 2680, 311, 1510, 4455, 13, 4815, 1271, 636, 279, 1455, 3293, 323, 13687, 2038, 11, 358, 7079, 13598, 264, 15062, 3754, 2592, 477, 279, 4033, 3997, 315, 279, 5929, 4783, 369, 279, 5652, 9013, 389, 279, 32858, 13, 128009], tokens=['As', ' of', ' my', ' knowledge', ' cutoff', ' in', ' ', '202', '3', ',', ' the', ' President', ' of', ' the', ' United', ' States', ' was', ' Joe', ' Biden', '.', ' However', ',', ' please', ' note', ' that', ' my', ' information', ' may', ' not', ' be', ' up', ' to', ' date', ',', ' and', ' I', ' do', ' not', ' have', ' real', '-time', ' access', ' to', ' current', ' events', '.', ' \\n\\n', 'To', ' get', ' the', ' most', ' recent', ' and', ' accurate', ' information', ',', ' I', ' recommend', ' checking', ' a', ' reliable', ' news', ' source', ' or', ' the', ' official', ' website', ' of', ' the', ' White', ' House', ' for', ' the', ' latest', ' updates', ' on', ' the', ' presidency', '.', '<|eot_id|>'], token_logprobs=[-0.875, -0.31445312, -0.0065307617, -0.46484375, -0.0074768066, -0.15136719, -0.578125, -0.00015640259, -1.3113022e-06, -0.00015258789, -0.09472656, -0.060058594, -4.7683716e-06, -9.655952e-06, -7.1525574e-07, -3.33786e-06, -0.5234375, -0.0014648438, -2.3841858e-07, -0.0031585693, -0.29492188, -0.004486084, -0.265625, -0.0028686523, -7.1525574e-05, -0.46484375, -0.16113281, -0.18554688, -0.25390625, -0.06982422, -0.006439209, -0.42773438, -4.5776367e-05, -0.69921875, -0.049804688, -1.0078125, -0.79296875, -0.0023651123, -0.014526367, -0.23730469, -1.1563301e-05, -0.328125, -0.07519531, -0.12695312, -0.059570312, -0.8828125, -1.015625, -0.5703125, -0.14746094, -0.002105713, -0.020874023, -0.76171875, -0.34960938, -0.0061950684, -0.00063323975, -0.100097656, -0.017700195, -0.35546875, -0.0067749023, -0.6328125, -0.023925781, -0.0047302246, -0.00029945374, -0.06225586, -0.3203125, -0.001701355, -0.18652344, -0.00049209595, -0.0002975464, -0.020141602, -9.059906e-06, -0.76171875, -0.020263672, -0.578125, -0.21386719, -0.16308594, -0.0018539429, -0.78515625, -0.037353516, -0.041748047]))], usage=Usage(completion_tokens=80, prompt_tokens=44, total_tokens=124, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt=[{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 February 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho is the president of the United States?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'logprobs': {'token_ids': [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1187, 7552, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 15546, 374, 279, 4872, 315, 279, 3723, 4273, 30, 128009, 128006, 78191, 128007, 271], 'tokens': ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '24', ' February', ' ', '202', '5', '\\n\\n', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'Who', ' is', ' the', ' president', ' of', ' the', ' United', ' States', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n'], 'token_logprobs': [None, -5.34375, -24.5, -2.359375, -15.5, -11.9375, -1.1875, -7.34375, -8.625, -0.034179688, -2.390625, -6.198883e-05, -2.796875, -5.625, -1.0625, -9.25, -8.8125, -0.00035476685, -0.29492188, -1.796875, -4.71875, -0.12890625, -0.022827148, -4.15625, -0.03564453, -20.75, -1.1920929e-07, -33.25, -0.67578125, -4.005432e-05, -7.875, -1.0625, -1.0390625, -4.09375, -0.015136719, -0.76953125, -0.59765625, -0.045654297, -0.67578125, -4.125, -1.1920929e-07, 0, 0, 0]}}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, _, _, _ = get_litellm_response(\"Who is the president of the United States?\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:41:27 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-24 17:41:27 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:41:27 model_runner.py:1110] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n",
      "INFO 02-24 17:41:27 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 02-24 17:41:27 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.15it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:41:28 model_runner.py:1115] Loading model weights took 0.6748 GB\n",
      "INFO 02-24 17:41:28 worker.py:267] Memory profiling takes 0.32 seconds\n",
      "INFO 02-24 17:41:28 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n",
      "INFO 02-24 17:41:28 worker.py:267] model weights take 0.67GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 34.31GiB.\n",
      "INFO 02-24 17:41:29 executor_base.py:111] # cuda blocks: 56208, # CPU blocks: 6553\n",
      "INFO 02-24 17:41:29 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 109.78x\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 39.38 GiB of which 789.38 MiB is free. Process 1007266 has 38.59 GiB memory in use. Of the allocated memory 37.77 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 150.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuggingFaceTB/SmolLM2-360M-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:489\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:276\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:434\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    436\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit engine (profile, create kv cache, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup model) took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m), elapsed)\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/executor/executor_base.py:122\u001b[0m, in \u001b[0;36mExecutorBase.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitialize_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py:56\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 56\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/utils.py:2196\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/worker/worker.py:306\u001b[0m, in \u001b[0;36mWorker.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    304\u001b[0m     context \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warm_up_model()\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/worker/worker.py:311\u001b[0m, in \u001b[0;36mWorker._init_cache_engine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_parallel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine[ve]\u001b[38;5;241m.\u001b[39mgpu_cache\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[1;32m    320\u001b[0m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompilation_config\u001b[38;5;241m.\u001b[39mstatic_forward_context,\n\u001b[1;32m    321\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/worker/worker.py:312\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    315\u001b[0m     ]\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine[ve]\u001b[38;5;241m.\u001b[39mgpu_cache\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[1;32m    320\u001b[0m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompilation_config\u001b[38;5;241m.\u001b[39mstatic_forward_context,\n\u001b[1;32m    321\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/worker/cache_engine.py:69\u001b[0m, in \u001b[0;36mCacheEngine.__init__\u001b[0;34m(self, cache_config, model_config, parallel_config, device_config)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend \u001b[38;5;241m=\u001b[39m get_attn_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size,\n\u001b[1;32m     62\u001b[0m                                      model_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     63\u001b[0m                                      cache_config\u001b[38;5;241m.\u001b[39mcache_dtype,\n\u001b[1;32m     64\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m     65\u001b[0m                                      model_config\u001b[38;5;241m.\u001b[39mis_attention_free,\n\u001b[1;32m     66\u001b[0m                                      use_mla\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39muse_mla)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Initialize the cache.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allocate_kv_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allocate_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cpu_blocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/klarity/.venv/lib/python3.11/site-packages/vllm/worker/cache_engine.py:103\u001b[0m, in \u001b[0;36mCacheEngine._allocate_kv_cache\u001b[0;34m(self, num_blocks, device)\u001b[0m\n\u001b[1;32m     97\u001b[0m     alloc_shape \u001b[38;5;241m=\u001b[39m kv_cache_shape\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_layers):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# null block in CpuGpuBlockAllocator requires at least that\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# block to be zeroed-out.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# We zero-out everything for simplicity.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     layer_kv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43malloc_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# If we allocated with padding for alignment reasons truncate the\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# shape while preserving the aligned stride\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malign_cache:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 39.38 GiB of which 789.38 MiB is free. Process 1007266 has 38.59 GiB memory in use. Of the allocated memory 37.77 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 150.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=128,\n",
    "    temperature=0.0,\n",
    "    logprobs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vllm_response(\n",
    "    llm: LLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    sampling_params: SamplingParams, \n",
    "    text: str,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    mean_entropy = []\n",
    "    mean_semantic_entropy = []\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\\\n",
    "            You are a question answering assistant. Respond with only the answer and no other context\"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    vllm_response = llm.generate(input_text, sampling_params)\n",
    "    answer = vllm_response[0].outputs[0].text\n",
    "    analysis_results = uncertainty_estimator.analyze_generation(vllm_response[0])\n",
    "    for token_metric in analysis_results.token_metrics:\n",
    "        mean_entropy.append(token_metric.raw_entropy)\n",
    "        mean_semantic_entropy.append(token_metric.semantic_entropy)\n",
    "    return answer, analysis_results, np.mean(mean_entropy), np.mean(mean_semantic_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 28.46it/s, est. speed input: 1054.40 toks/s, output: 227.92 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sample_queries = [\"What is the capital of France?\", \"What is the capital of Spain?\"]\n",
    "answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, sample_queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Spain is Madrid.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\",\n",
       " 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
       " 'right_answer': \"Arthur's Magazine\",\n",
       " 'hallucinated_answer': 'First for Women was started first.',\n",
       " 'task_type': 'QA'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "predicted_answers = []\n",
    "mean_entropies = []\n",
    "mean_semantic_entropies = []\n",
    "correct_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\",\n",
       " 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\",\n",
       " 'right_answer': \"Arthur's Magazine\",\n",
       " 'hallucinated_answer': 'First for Women was started first.',\n",
       " 'task_type': 'QA'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 44.16it/s, est. speed input: 4397.24 toks/s, output: 177.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.28it/s, est. speed input: 1276.46 toks/s, output: 252.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 31.23it/s, est. speed input: 5568.46 toks/s, output: 218.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 17.42it/s, est. speed input: 3034.36 toks/s, output: 261.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 34.64it/s, est. speed input: 3678.09 toks/s, output: 208.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 15.89it/s, est. speed input: 2704.12 toks/s, output: 254.47 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 46.06it/s, est. speed input: 7063.45 toks/s, output: 184.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 39.74it/s, est. speed input: 9955.34 toks/s, output: 199.03 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s, est. speed input: 1233.88 toks/s, output: 272.01 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s, est. speed input: 1893.17 toks/s, output: 270.43 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 45.88it/s, est. speed input: 6713.24 toks/s, output: 183.84 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 17.90it/s, est. speed input: 3225.38 toks/s, output: 250.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s, est. speed input: 1296.22 toks/s, output: 272.03 toks/s]\n"
     ]
    }
   ],
   "source": [
    "def check_answers(predicted_answer, correct_answer):\n",
    "    # Check for exact match first\n",
    "    if predicted_answer.strip().lower() == correct_answer.strip().lower():\n",
    "        return True\n",
    "    \n",
    "    # If not an exact match, use JudgeLLM\n",
    "    prompt = f\"\"\"\n",
    "    Question: Are these two answers equivalent in meaning?\n",
    "    Answer 1: {predicted_answer}\n",
    "    Answer 2: {correct_answer}\n",
    "    Please respond with only 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, judge_response, _, _ = get_litellm_response(prompt)\n",
    "    return judge_response.strip().lower() == 'yes'\n",
    "\n",
    "accuracy = []\n",
    "# Update the main loop\n",
    "for item in tqdm(ds):\n",
    "    correct_answers.append(item['right_answer'])\n",
    "    combined_text = f\"Context: {item['knowledge']}\\nQuestion: {item['question']}\\n Answer:\"\n",
    "    predicted_answer, result, mean_entropy, mean_semantic_entropy = get_vllm_response(llm, tokenizer, sampling_params, combined_text)\n",
    "    predicted_answers.append(predicted_answer)\n",
    "    mean_entropies.append(mean_entropy)\n",
    "    mean_semantic_entropies.append(mean_semantic_entropy)\n",
    "    \n",
    "    # Check if the answers match\n",
    "    is_correct = check_answers(predicted_answer, item['right_answer'])\n",
    "    accuracy.append(is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UncertaintyMetrics(raw_entropy=0.5193799723392782, semantic_entropy=0.5193799723392782, token_predictions=[TokenInfo(token='First', token_id=5345, logit=-0.4102168381214142, probability=0.6635063610636857, attention_score=None), TokenInfo(token='Arthur', token_id=27037, logit=-1.5352168083190918, probability=0.2154089836469735, attention_score=None), TokenInfo(token='B', token_id=50, logit=-2.910216808319092, probability=0.05446392035801202, attention_score=None), TokenInfo(token='Both', token_id=12857, logit=-4.535216808319092, probability=0.010724581795883955, attention_score=None), TokenInfo(token='Only', token_id=15017, logit=-5.035216808319092, probability=0.006504787671799594, attention_score=None)], insight=None, attention_metrics=None),\n",
       " UncertaintyMetrics(raw_entropy=0.011020957731614189, semantic_entropy=0.0006636399204034579, token_predictions=[TokenInfo(token=' for', token_id=327, logit=-0.0026856327895075083, probability=0.9973179702959892, attention_score=None), TokenInfo(token=' For', token_id=1068, logit=-6.127685546875, probability=0.002181624389459836, attention_score=None), TokenInfo(token='for', token_id=1710, logit=-8.815185546875, probability=0.00014846140286453328, attention_score=None), TokenInfo(token='.', token_id=30, logit=-10.315185546875, probability=3.3126216597023867e-05, attention_score=None), TokenInfo(token=' Women', token_id=6038, logit=-10.377685546875, probability=3.111920059283279e-05, attention_score=None)], insight=None, attention_metrics=None),\n",
       " UncertaintyMetrics(raw_entropy=0.002316005072438052, semantic_entropy=0.000573664230853115, token_predictions=[TokenInfo(token=' Women', token_id=6038, logit=-0.00039545822073705494, probability=0.9996046199625587, attention_score=None), TokenInfo(token=' Woman', token_id=20336, logit=-8.250395774841309, probability=0.00026115517819638484, attention_score=None), TokenInfo(token=' women', token_id=1554, logit=-9.875395774841309, probability=5.142450362689996e-05, attention_score=None), TokenInfo(token=' W', token_id=408, logit=-10.250395774841309, probability=3.534351000991581e-05, attention_score=None), TokenInfo(token='Women', token_id=17449, logit=-10.375395774841309, probability=3.119053811021834e-05, attention_score=None)], insight=None, attention_metrics=None),\n",
       " UncertaintyMetrics(raw_entropy=0.5904666284662318, semantic_entropy=0.5904666284662319, token_predictions=[TokenInfo(token='', token_id=2, logit=-0.4553496837615967, probability=0.6342261505814846, attention_score=None), TokenInfo(token=' was', token_id=436, logit=-1.4553496837615967, probability=0.23331876185223158, attention_score=None), TokenInfo(token='.', token_id=30, logit=-2.8303496837615967, probability=0.05899222144038213, attention_score=None), TokenInfo(token=' is', token_id=314, logit=-3.4553496837615967, probability=0.03157626071968755, attention_score=None), TokenInfo(token=',', token_id=28, logit=-4.580349922180176, probability=0.010251308508494402, attention_score=None)], insight=None, attention_metrics=None)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prinf(f\"Accuracy is {np.mean(accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e_thresh \u001b[38;5;129;01min\u001b[39;00m entropy_thresholds:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s_thresh \u001b[38;5;129;01min\u001b[39;00m semantic_thresholds:\n\u001b[0;32m---> 35\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m evaluate_metrics(\u001b[43mresults_df\u001b[49m, e_thresh, s_thresh)\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# Score based on high hallucination detection and low false positives\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         score \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhallucination_f1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse_positive_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "def plot_metrics(semantic_entropy, raw_entropy, labels):\n",
    "    # Calculate metrics\n",
    "    semantic_auroc = roc_auc_score(labels, semantic_entropy)\n",
    "    raw_auroc = roc_auc_score(labels, raw_entropy)\n",
    "    \n",
    "    semantic_precision, semantic_recall, _ = precision_recall_curve(labels, semantic_entropy)\n",
    "    raw_precision, raw_recall, _ = precision_recall_curve(labels, raw_entropy)\n",
    "    \n",
    "    semantic_pr_auc = auc(semantic_recall, semantic_precision)\n",
    "    raw_pr_auc = auc(raw_recall, raw_precision)\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random')\n",
    "    plt.plot(semantic_recall, semantic_precision, label=f'Semantic (AUC = {semantic_pr_auc:.2f})')\n",
    "    plt.plot(raw_recall, raw_precision, label=f'Raw (AUC = {raw_pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot PR curves\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(semantic_entropy, bins=50, alpha=0.5, label='Semantic')\n",
    "    plt.hist(raw_entropy, bins=50, alpha=0.5, label='Raw')\n",
    "    plt.xlabel('Entropy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Entropy Distribution')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    semantic_accuracy = [accuracy_score(labels, semantic_entropy > t) for t in thresholds]\n",
    "    raw_accuracy = [accuracy_score(labels, raw_entropy > t) for t in thresholds]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(thresholds, semantic_accuracy, label='Semantic')\n",
    "    plt.plot(thresholds, raw_accuracy, label='Raw')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Threshold')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(mean_semantic_entropies, mean_entropies, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
